#直接random.shuffer将数据分成了9:1，也可以用skf多折划分数据。
#将标题 出处 描述三种文本分别加载roberta tokenizer，然后cat起来输入
#数据量太少简单用了repeat增强，即将训练数据复制了一次。
import json
import torch
from torch.utils.data import DataLoader,Dataset,RandomSampler,SequentialSampler
from transformers import BertTokenizer
from transformers.data.data_collator import DataCollatorForLanguageModeling


def create_dataloaders(args):

    #读取训练集数据并切分
    train_anns=list()
    with open(args.train_file, 'r', encoding='utf8') as f:
        for line in f.readlines():
            ann = json.loads(line)
            train_anns.append(ann)

    val_anns = list()
    with open(args.valid_file, 'r', encoding='utf8') as f:
        for line in f.readlines():
            ann = json.loads(line)
            val_anns.append(ann)

    #简单的数据增强：repeat
    #train_anns = train_anns + train_anns
    tokenizer = BertTokenizer.from_pretrained(args.bert_dir)
    data_collator = DataCollatorForLanguageModeling(
                        tokenizer=tokenizer,
                        mlm = True, 
                        mlm_probability= 0.15,
                        pad_to_multiple_of=None,
                    )


    val_dataset = MultiModalDataset(args, val_anns, tokenizer)
    train_dataset = MultiModalDataset(args, train_anns, tokenizer)
    train_sampler = RandomSampler(train_dataset)
    val_sampler = SequentialSampler(val_dataset)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size,sampler=train_sampler,collate_fn=data_collator,
                                    drop_last=True, pin_memory=True, num_workers=args.num_workers, prefetch_factor=args.prefetch)
    val_dataloader = DataLoader(val_dataset,batch_size=args.val_batch_size,sampler=val_sampler,collate_fn=data_collator,
                                    drop_last=False,pin_memory=True,num_workers=args.num_workers,prefetch_factor=args.prefetch)

    return train_dataloader, val_dataloader


class MultiModalDataset(Dataset):#处理原始数据集
    def __init__(self, args, anns, tokenizer, test_mode:bool=False,idx=[]):
        self.test_mode = test_mode

        self.tokenizer = tokenizer
        self.label2id = [{"admin/root":0,"nonprivileged":1,"access":2,"unknown":3},
                        {"remote":0, "non-remote":1},
                        {'privileged-gained(rce)_admin/root':0,'privileged-gained(rce)_nonprivileged':1,'privileged-gained(rce)_unknown':2,
                        'information-disclosure_local(credit)_admin/root':3,'information-disclosure_local(credit)_nonprivileged':4,
                        'information-disclosure_local(credit)_unknown':5,'information-disclosure_other-target(credit)_admin/root':6,
                        'information-disclosure_other-target(credit)_nonprivileged':7,'information-disclosure_other-target(credit)_unknown':8,
                        'information-disclosure_other':9,'dos':10,'access':11,'other':12}]#对应三种label
        self.id2label = [{v:k for k,v in label_map.items()} for label_map in self.label2id]
    
        self.anns = anns

    def __len__(self) -> int:
        return len(self.anns)
    
    def __getitem__(self, idx:int) -> dict:
        id = self.anns[idx]['cve-number']
        description = self.anns[idx]['description']

        text_inputs = self.tokenizer(description, max_length=512, padding='max_length', truncation=True)
        
        text_inputs = {k: torch.LongTensor(v) for k,v in text_inputs.items()}

        text_mask = text_inputs['attention_mask']
        data=dict(input_ids=text_inputs['input_ids'], text_inputs=text_inputs['input_ids'], attention_mask=text_mask, token_type_ids=text_inputs['token_type_ids'])

        if (not self.test_mode) and 'privilege-required' in self.anns[idx]:#写成三列
            data['main_label'] = torch.LongTensor([self.label2id[0][self.anns[idx]['privilege-required']], self.label2id[1][self.anns[idx]['attack-vector']],self.label2id[2][self.anns[idx]['impact']]])

        return data