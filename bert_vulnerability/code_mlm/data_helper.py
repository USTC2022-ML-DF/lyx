#直接random.shuffer将数据分成了9:1，也可以用skf多折划分数据。
#将标题 出处 描述三种文本分别加载roberta tokenizer，然后cat起来输入
#数据量太少简单用了repeat增强，即将训练数据复制了一次。
import json
import torch
import random
import math
from torch.utils.data import DataLoader,Dataset,RandomSampler,SequentialSampler
from torch.utils.data.dataloader import default_collate

from transformers import RobertaTokenizer
from transformers.data.data_collator import DataCollatorForLanguageModeling


def load_json_list(file_path):
    with open(file_path, 'r', encoding='utf8') as f:
        return [json.loads(line) for line in filter(lambda l: l.strip() != '', f.readlines())]


def create_dataloaders(args):

    # 读取训练集数据并切分
    train_anns = load_json_list(args.train_file)
    val_anns = load_json_list(args.valid_file)

    mlm_train_anns = load_json_list(args.mlm_train_file)
    mlm_val_anns = load_json_list(args.mlm_valid_file)

    # # 数据对齐,eval集没必要扩充，默认数目一致
    # train_anns = train_anns * math.ceil(len(mlm_train_anns)/len(train_anns))
    # mlm_train_anns += random.sample(mlm_train_anns, len(train_anns)-len(mlm_train_anns))

    tokenizer = RobertaTokenizer.from_pretrained(args.bert_dir)

    # For CLS data
    cls_val_dataset = CLSDataset(args, tokenizer, val_anns)
    cls_train_dataset = CLSDataset(args, tokenizer, train_anns)

    cls_train_dataloader = DataLoader(
        cls_train_dataset,
        sampler=RandomSampler(cls_train_dataset),
        batch_size=args.batch_size,
        collate_fn=cls_train_dataset.collate_fn,
    )

    cls_val_dataloader = DataLoader(
        cls_val_dataset,
        sampler=SequentialSampler(cls_val_dataset),
        batch_size=args.batch_size,
        collate_fn=cls_val_dataset.collate_fn,
    )

    # For MLM data
    mlm_val_dataset = MLMDataset(args, tokenizer, mlm_val_anns)
    mlm_train_dataset = MLMDataset(args, tokenizer, mlm_train_anns)

    mlm_train_dataloader = DataLoader(
        mlm_train_dataset,
        sampler=RandomSampler(mlm_train_dataset),
        batch_size=args.batch_size,
        collate_fn=mlm_train_dataset.collate_fn
    )

    mlm_val_dataloader = DataLoader(
        mlm_val_dataset,
        sampler=SequentialSampler(mlm_val_dataset),
        batch_size=args.batch_size,
        collate_fn=mlm_val_dataset.collate_fn
    )

    return cls_train_dataloader, cls_val_dataloader, mlm_train_dataloader, mlm_val_dataloader


class CLSDataset(Dataset):  
    def __init__(self, args, tokenizer, anns, test_mode: bool = False):
        self.test_mode = test_mode

        self.tokenizer = tokenizer
        self.label2id = {
            'privilege-required': {
                "admin/root": 0, 
                "nonprivileged": 1, 
                "access": 2, 
                "unknown": 3
            },
            'attack-vector': {
                "remote": 0,
                "non-remote": 1
            },
            'impact': {
                'privileged-gained(rce)_admin/root': 0,
                'privileged-gained(rce)_nonprivileged': 1,
                'privileged-gained(rce)_unknown': 2,
                'information-disclosure_local(credit)_admin/root': 3,
                'information-disclosure_local(credit)_nonprivileged': 4,
                'information-disclosure_local(credit)_unknown': 5,
                'information-disclosure_other-target(credit)_admin/root': 6,
                'information-disclosure_other-target(credit)_nonprivileged': 7,
                'information-disclosure_other-target(credit)_unknown': 8,
                'information-disclosure_other': 9,
                'dos': 10,
                'access': 11,
                'other': 12
            }
        }

        self.id2label = {
            column: {v: k for k, v in label_map.items()} for column, label_map in self.label2id.items()
        }

        self.anns = anns

        self.default_collator = default_collate

    def __len__(self) -> int:
        return len(self.anns)
    
    def __getitem__(self, idx:int) -> dict:
        description = self.anns[idx]['description']

        text_inputs = self.tokenizer(description, max_length=256, padding='max_length', truncation=True)
        
        data = {
            'input_ids': torch.tensor(text_inputs['input_ids'], dtype=torch.long), 
            'attention_mask': torch.tensor(text_inputs['attention_mask'], dtype=torch.float32)
        }

        if not self.test_mode:  #写成三列
            #data['main_label'] = torch.LongTensor([self.label2id[0][self.anns[idx]['privilege-required']], self.label2id[1][self.anns[idx]['attack-vector']],self.label2id[2][self.anns[idx]['impact']]])
            for k in self.label2id.keys():
                data[k] = torch.tensor(self.label2id[k][self.anns[idx][k]], dtype=torch.long)
    
        return data
    
    def collate_fn(self, batch):
        batch = self.default_collator(batch)
        batch['task'] = 'cls'
        return batch


class MLMDataset(Dataset):
    def __init__(self, args, tokenizer, anns):
        self.tokenizer = tokenizer

        self.anns = anns
        
        self.mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

    def __len__(self) -> int:
        return len(self.anns)
    
    def __getitem__(self, idx:int) -> dict:
        description = self.anns[idx]['description']

        text_inputs = self.tokenizer(description, max_length=256, padding='max_length', truncation=True)
        
        data = {
            'input_ids': torch.tensor(text_inputs['input_ids'], dtype=torch.long), 
            'attention_mask': torch.tensor(text_inputs['attention_mask'], dtype=torch.float32)
        }

        return data

    def collate_fn(self, batch):
        batch = self.mlm_collator(batch)
        batch['task'] = 'mlm'
        return batch